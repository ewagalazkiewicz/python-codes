{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff1a30cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Batch Gradient Descent with early stopping for Softmax Regression\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a5c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f2a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8b1f22ed4f18>:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = (iris[\"target\"] == 2).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "X = iris[\"data\"][:, (2,3)]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cdc315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.2590\n",
      "Epoch 200/1000, Loss: 0.1981\n",
      "Epoch 300/1000, Loss: 0.1702\n",
      "Epoch 400/1000, Loss: 0.1536\n",
      "Epoch 500/1000, Loss: 0.1424\n",
      "Epoch 600/1000, Loss: 0.1343\n",
      "Epoch 700/1000, Loss: 0.1281\n",
      "Epoch 800/1000, Loss: 0.1232\n",
      "Epoch 900/1000, Loss: 0.1192\n",
      "Epoch 1000/1000, Loss: 0.1159\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 2: Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Add a bias term to the feature matrix\n",
    "X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "# Step 5: Define the softmax function\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return e_z / e_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Step 6: Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred)) / m\n",
    "\n",
    "# Step 7: Initialize model parameters\n",
    "num_classes = 3\n",
    "num_features = X_train.shape[1]\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "\n",
    "weights = np.random.randn(num_features, num_classes)\n",
    "\n",
    "# Step 8: Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    scores = np.dot(X_train, weights)\n",
    "    predicted_probs = softmax(scores)\n",
    "    loss = cross_entropy_loss(np.eye(num_classes)[y_train], predicted_probs)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradients = np.dot(X_train.T, (predicted_probs - np.eye(num_classes)[y_train])) / X_train.shape[0]\n",
    "\n",
    "    # Update weights\n",
    "    weights -= learning_rate * gradients\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Step 9: Use the trained model for inference on the test set\n",
    "test_scores = np.dot(X_test, weights)\n",
    "test_predicted_probs = softmax(test_scores)\n",
    "test_predicted_labels = np.argmax(test_predicted_probs, axis=1)\n",
    "\n",
    "# Step 10: Calculate and print accuracy\n",
    "correct_predictions = (test_predicted_labels == y_test)\n",
    "accuracy = np.mean(correct_predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085e94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
